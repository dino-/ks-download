- Figure out what the top-level "tools" should be. What is the
  progression of tasks to go from nothing to inspections scraped to
  data inserted into the db?
- For above, need a set of dirs input or in Config that represent
  where data is coming from and going to.
- IMPORTANT: we are not saving or moving the inspection files from
  an input dir to a failed dir.
- When saving data files, use some prefix conventions, like:
   - insp_UUID.json  -- "raw" inspection data
   - ks_UUID.json    -- document for Couch
   - ??.log          -- Log of running inspection-to-places lookups
- Write some usage docs in README.md
- It may be time to put info in changelog.md
- Look into accessing CouchBase directly from Haskell
- Is there a different url encoding API than what's in Network.HTTP?
- ks-inspections
   - Change the main.downloader implementation into a (Map String
     Downloader) so it can be used in usage output
   - Do proper usage output
- ks-location
   - Do proper usage output
   - Look for Places API key file in /etc/ as well as $HOME
- Ksdl.Inspection
   - Should we really be using Data.ByteString.Lazy.Char8 for
     saveInspection? Try with just Data.ByteString.Lazy or even
     not lazy.
- Ksdl.Inspection.*
   - Replace usage of Network.HTTP with Network.HTTP.Conduit
     (http-conduit). See Ksdl.Geocoding/Places
- Ksdl.Inspection.NcWake
   - Need to get more data from here for each inspection:
      - Violations and critical violations
- Ksdl.Places.Match
   - Ditch the csv business and save out the JSON documents for
     the db. For good AND bad, into different dirs (see above)
      - good: DbJson saved
      - bad: Inspection saved
